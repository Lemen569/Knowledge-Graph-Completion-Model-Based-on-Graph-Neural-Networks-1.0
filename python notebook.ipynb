{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e80df80f-a70d-4a23-ab51-b3c6be3ea297",
   "metadata": {},
   "source": [
    "Step 1: Load the Pre-split Data\n",
    "training, validation, and test data are already split and stored in train.txt, valid.txt, and test.txt, we will load these datasets.(FB15K FB15k-237 NELL WN18RR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7698a277-a3d5-4fc1-9d5d-ddf1a480228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the pre-split training, validation, and test data\n",
    "train_data_loader = pd.read_csv('data/train.txt', sep=' ', header=None, names=['head', 'relation', 'tail'])\n",
    "valid_data_loader = pd.read_csv('data/valid.txt', sep=' ', header=None, names=['head', 'relation', 'tail'])\n",
    "test_data_loader = pd.read_csv('data/test.txt', sep=' ', header=None, names=['head', 'relation', 'tail'])\n",
    "\n",
    "# Print the head of the loaded datasets to ensure correctness\n",
    "print(\"Training Data Head:\", train_data_loader.head())\n",
    "print(\"Validation Data Head:\", valid_data_loader.head())\n",
    "print(\"Test Data Head:\", test_data_loader.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992acb8b-02ae-439a-b4ce-a0239013c209",
   "metadata": {},
   "source": [
    "Data Processing Explanation:\n",
    "Data Source: train.txt, valid.txt, and test.txt files, each containing triplet data (head, relation, tail).\n",
    "Data Processing: These files are loaded as pandas DataFrames, and columns are named (head, relation, tail). This data will be used for model training, validation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218cad5e-2675-436a-b123-11532e7a852e",
   "metadata": {},
   "source": [
    "Step 2: Data Preprocessing\n",
    "Before training, ensure that there are no missing values by removing rows containing NaN values.\n",
    "Use the IQR (Interquartile Range) method to detect outliers. IQR is the range of the middle 50% of the data distribution. Any data points smaller than Q1 minus 1.5 times the IQR, or larger than Q3 plus 1.5 times the IQR, are considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac61a6d2-893a-443b-9675-2698a2ddd530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "train_data_loader = train_data_loader.dropna()\n",
    "valid_data_loader = valid_data_loader.dropna()\n",
    "test_data_loader = test_data_loader.dropna()\n",
    "# Identify and remove outliers using IQR method\n",
    "Q1 = train_data_loader['head'].quantile(0.25)\n",
    "Q3 = train_data_loader['head'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "train_data_loader = train_data_loader[(train_data_loader['head'] >= (Q1 - 1.5 * IQR)) & \n",
    "                                      (train_data_loader['head'] <= (Q3 + 1.5 * IQR))]\n",
    "\n",
    "# Repeat for 'tail' and 'relation' columns\n",
    "\n",
    "# Print the sizes of the datasets after dropping NaN values\n",
    "print(f\"Training Data Size after Dropping NA: {len(train_data_loader)}\")\n",
    "print(f\"Validation Data Size after Dropping NA: {len(valid_data_loader)}\")\n",
    "print(f\"Test Data Size after Dropping NA: {len(test_data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8baec-beb9-40b5-90d7-d452adfbfee4",
   "metadata": {},
   "source": [
    "Data Processing Explanation:\n",
    "Data Source: From the train_data_loader, valid_data_loader, and test_data_loader datasets.\n",
    "Data Processing: Rows containing NaN values are removed to ensure that each triplet (head, relation, tail) is complete, preventing issues during training.\n",
    "Data Usage: Cleaned datasets will be used for training, validation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd3f30-3d66-41b9-8840-f2ad9f0be266",
   "metadata": {},
   "source": [
    "Step 3: Negative Sample Generation (Different Methods for Different Models)\n",
    "For TransE and DistMult decoders, we use Bernoulli Negative Sampling.\n",
    "For RotatE decoder, we use Self-Adversarial Negative Sampling.\n",
    "Bernoulli Negative Sampling (for TransE and DistMult):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0c65d1-0d82-46ad-9c3f-4fdd6878c516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Bernoulli Negative Sampling: For TransE and DistMult\n",
    "def bernoulli_negative_sampling(train_data_loader, num_negatives=5):\n",
    "    negative_samples = []\n",
    "    entities = train_data_loader['head'].unique()  # Get all entities\n",
    "    relations = train_data_loader['relation'].unique()  # Get all relations\n",
    "\n",
    "    for _, row in train_data_loader.iterrows():\n",
    "        head, relation, tail = row['head'], row['relation'], row['tail']\n",
    "        \n",
    "        for _ in range(num_negatives):  # Generate num_negatives negative samples per positive sample\n",
    "            if np.random.rand() < 0.5:  # 50% chance to replace the head entity\n",
    "                negative_head = np.random.choice(entities)\n",
    "                negative_samples.append((negative_head, relation, tail))\n",
    "            else:  # 50% chance to replace the tail entity\n",
    "                negative_tail = np.random.choice(entities)\n",
    "                negative_samples.append((head, relation, negative_tail))\n",
    "\n",
    "    return pd.DataFrame(negative_samples, columns=['head', 'relation', 'tail'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a183318-807f-41b7-bd9a-36d9b3fbca65",
   "metadata": {},
   "source": [
    "Data Processing Explanation:\n",
    "Training Data Source: train_data_loader, which contains the positive samples from the training set.\n",
    "Data Processing: Bernoulli Negative Sampling is applied to generate negative samples. We randomly choose to replace either the head or tail entity to generate the negative sample.\n",
    "Data Usage: The generated negative samples are combined with the original training data to form a dataset that will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b126628-a7c9-4812-b663-351113ef9c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Adversarial Negative Sampling: For RotatE\n",
    "def self_adversarial_negative_sampling(train_data_loader, model, num_negatives=5, temperature=1.0):\n",
    "    negative_samples = []\n",
    "    entities = train_data_loader['head'].unique()\n",
    "\n",
    "    for _, row in train_data_loader.iterrows():\n",
    "        head, relation, tail = row['head'], row['relation'], row['tail']\n",
    "        candidates = []\n",
    "\n",
    "        # Generate candidate negative samples\n",
    "        for _ in range(num_negatives * 10):  # Generate multiple candidate negative samples\n",
    "            if np.random.rand() < 0.5:\n",
    "                negative_head = np.random.choice(entities)\n",
    "                candidates.append((negative_head, relation, tail))\n",
    "            else:\n",
    "                negative_tail = np.random.choice(entities)\n",
    "                candidates.append((head, relation, negative_tail))\n",
    "\n",
    "        # Use the model's scoring method to calculate the scores of the candidate negative samples\n",
    "        scores = model.score_candidates(candidates)  # Assuming the model has a score_candidates method\n",
    "        scores = torch.softmax(scores / temperature, dim=0)  # Adjust based on temperature\n",
    "        selected_indices = torch.multinomial(scores, num_negatives, replacement=True)\n",
    "\n",
    "        for idx in selected_indices:\n",
    "            negative_samples.append(candidates[idx])\n",
    "\n",
    "    return pd.DataFrame(negative_samples, columns=['head', 'relation', 'tail'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa4909-074f-459b-9663-92e8dcb8cbac",
   "metadata": {},
   "source": [
    "Data Processing Explanation:\n",
    "Training Data Source: train_data_loader, which contains the positive samples from the training set.\n",
    "Data Processing: Self-Adversarial Negative Sampling is applied. Multiple negative candidates are generated, and their scores are computed using the model itself. Based on the scores, negative samples are selected using the softmax function and temperature scaling.\n",
    "Data Usage: The selected negative samples are combined with the original positive samples and will be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21af428-42c7-4564-a7e7-418fd0ecfc44",
   "metadata": {},
   "source": [
    "Step 4: Model Training\n",
    "We initialize the corresponding model based on the decoder choice and proceed with training, using the generated negative samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2045a53a-7648-44c1-97b3-9e6830cd358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from model import TransEModel, DistMultModel, RotatEModel  # Import your selected model\n",
    "\n",
    "# Choose decoder (model)\n",
    "model_choice = \"RotatE\"  # Can choose \"TransE\", \"DistMult\", \"RotatE\"\n",
    "\n",
    "# Initialize the corresponding model\n",
    "if model_choice == \"TransE\":\n",
    "    model = TransEModel(num_entities, num_relations, embedding_dim)\n",
    "elif model_choice == \"DistMult\":\n",
    "    model = DistMultModel(num_entities, num_relations, embedding_dim)\n",
    "else:  # Default to RotatE model\n",
    "    model = RotatEModel(num_entities, num_relations, embedding_dim)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()  # Using cross-entropy loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
    "\n",
    "# Training process  is described in detail in trainer.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e31c7e3-6362-4677-b251-e4753adf4ca6",
   "metadata": {},
   "source": [
    "Data Processing Explanation:\n",
    "Training Data Source: train_data_loader, which contains both positive and negative samples.\n",
    "Data Usage: The model is trained using the training data, which now includes both positive and generated negative samples. The loss is computed and used to update model parameters through backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd49f24a-0d53-4303-b179-959b29ab500b",
   "metadata": {},
   "source": [
    "Step 5: Validation Set Usage\n",
    "At the end of each epoch, we use the validation data to evaluate the model and help with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e58771-c8f0-4900-a5e7-10a92dc2109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Phase: After each epoch, evaluate the model using the validation set\n",
    "model.eval()  \n",
    "\n",
    "val_loss = 0  # Initialize validation loss\n",
    "for batch in valid_data.values:  # Validation data comes from the valid_data set\n",
    "    head, relation, tail = batch[0], batch[1], batch[2]\n",
    "    score = model(head, relation, tail)\n",
    "    val_loss += loss.item()  # Accumulate validation loss\n",
    "\n",
    "print(f\"Validation Loss: {val_loss}\")  # Print validation loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5329244a-aaa3-4173-812e-2eec39b83359",
   "metadata": {},
   "source": [
    "Data Processing Explanation:\n",
    "Validation Data Source: valid_data_loader, which comes from the validation set.\n",
    "Data Usage: At the end of each epoch, we evaluate the model on the validation set, compute the validation loss, and print it to monitor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e2ba4a-93d2-4408-a044-751be8348c3a",
   "metadata": {},
   "source": [
    "Step 6: Test Set Usage\n",
    "Finally, after training, we evaluate the model on the test set to assess.\n",
    "Testing process  is described in detail in Tester.py.\n",
    "\n",
    "Data Processing Explanation:\n",
    "Test Data Source: test_data_loader, which comes from the test set.\n",
    "Data Usage: The model is evaluated on the test data to assess how well it generalizes to unseen data, and the final test loss is computed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a7fba-76a2-4d72-b44d-eb46921bd134",
   "metadata": {},
   "source": [
    "This notebook only provides a detailed explanation of data processing and its usage in training, validation, and testing. The full implementation can be found in the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
